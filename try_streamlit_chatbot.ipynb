{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "# Function to get response from the model's API\n",
    "def get_model_response(user_input):\n",
    "    api_url = \"https://api-inference.huggingface.co/models/Intel/neural-chat-7b-v3-1\"  # Replace with the actual API URL\n",
    "    headers = {\"Authorization\": \"Bearer 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\"}\n",
    "    payload = {\"input\": user_input}\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['response']\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return \"Sorry, I couldn't fetch a response.\"\n",
    "\n",
    "# Streamlit webpage title\n",
    "st.title('Chatbot with Intel Neural Chat Model')\n",
    "\n",
    "# Create a sidebar for input\n",
    "with st.sidebar:\n",
    "    user_input = st.text_input(\n",
    "        \"Placeholder for the other text input widget\",\n",
    "        \"This is a placeholder\",\n",
    "        key=\"placeholder\",\n",
    "    )\n",
    "\n",
    "\n",
    "    if st.button('Send'):\n",
    "        bot_response = get_model_response(user_input)\n",
    "        st.sidebar.write(f\"Bot: {bot_response}\")\n",
    "\n",
    "# Main page\n",
    "st.write(\"Type your message in the sidebar and press send to chat!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "# Set padding side and pad token\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Chatbot initialized! Start talking to the bot (type 'quit' to stop).\")\n",
    "\n",
    "# Function to get response from the model's API\n",
    "def get_model_response(user_input):\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        exit()\n",
    "\n",
    "    # Encode the input and generate a response\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "    bot_input_ids = new_user_input_ids\n",
    "    bot_output = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and print the response\n",
    "    return tokenizer.decode(bot_output[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "# Streamlit webpage title\n",
    "st.title('Chatbot with Microsoft DiabloGPT Model')\n",
    "\n",
    "# Create a sidebar for input\n",
    "with st.sidebar:\n",
    "    user_input = st.text_input(\"You: \")\n",
    "\n",
    "    if st.button('Send'):\n",
    "        bot_response = get_model_response(user_input)\n",
    "        st.sidebar.write(f\"Bot: {bot_response}\")\n",
    "\n",
    "# Main page\n",
    "st.write(\"Type your message in the sidebar and press send to chat!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13343\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# def chatbot():\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     # Load the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m conversation_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversational\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/DialoGPT-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1343\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1341\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1343\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1344\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1353\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1352\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1353\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1356\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1357\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1358\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conversation, ConversationalPipeline\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdepth_estimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepthEstimationPipeline\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_question_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentQuestionAnsweringPipeline\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractionPipeline\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfill_mask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FillMaskPipeline\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\document_question_answering.py:29\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     ExplicitEnum,\n\u001b[0;32m     22\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     logging,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIPELINE_INIT_ARGS, ChunkPipeline\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_starts_ends\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\question_answering.py:9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, squad_convert_examples_to_features\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCard\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\data\\__init__.py:26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[0;32m     17\u001b[0m     DataCollatorForPermutationLanguageModeling,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     default_data_collator,\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     DataProcessor,\n\u001b[0;32m     29\u001b[0m     InputExample,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\data\\metrics\\__init__.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr, spearmanr\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, matthews_corrcoef\n\u001b[0;32m     23\u001b[0m DEPRECATION_WARNING \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrary. You can have a look at this example script for pointers: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_accuracy\u001b[39m(preds, labels):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\__init__.py:94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdet_curve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetCurveDisplay\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroc_curve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RocCurveDisplay\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprecision_recall_curve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrecisionRecallDisplay\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusion_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PredictionErrorDisplay\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def chatbot():\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "    # Set padding side and pad token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Chatbot initialized! Start talking to the bot (type 'quit' to stop).\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        # Encode the input and generate a response\n",
    "        new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "        bot_input_ids = new_user_input_ids\n",
    "        bot_output = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # Decode and print the response\n",
    "        response = tokenizer.decode(bot_output[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 19:38:20.153 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\13343\\anaconda3\\envs\\tf26\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "\n",
    "st.title(\"Chatbot with PHI 1.5\")\n",
    "user_input = st.text_input(\"Ask a question:\")\n",
    "submit_button = st.button(\"Submit\")\n",
    "if submit_button:\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", return_attention_mask=False)\n",
    "    outputs = model.generate(**inputs, max_length=200)\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    st.write(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last edited version using gpt 3.5 turbo as llm base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7c3e2d2deb4376abb4eaed70e5e80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/732 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fdc4410aef4b1faae2518319df42e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi.py:   0%|          | 0.00/2.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-1_5:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf70c9101b6d4d8f82e44706b1779716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi.py:   0%|          | 0.00/33.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-1_5:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f328237fe4f426f80eb0f70d71ce96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf4c54eaf2b42f8a98ff7d26d8a171e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a5cecfb6734b2fa9d569afb77d0ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e120cc9307324515a649e354f094ca5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453af4c15a1242e1ba624a4dc790b314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf673e9be1e4bde9b04b1e7ad87c2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a172a9364074432283847cc493f31982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e15ce88a284e3bafba3a3016c713fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   primes = []\n",
      "   for num in range(2, n+1):\n",
      "       is_prime = True\n",
      "       for i in range(2, int(math.sqrt(num))+1):\n",
      "           if num % i == 0:\n",
      "               is_prime = False\n",
      "               break\n",
      "       if is_prime:\n",
      "           primes.append(num)\n",
      "   print(primes)\n",
      "   \n",
      "print_prime(20)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "```\n",
      "\n",
      "Exercise 5:\n",
      "Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
      "\n",
      "```python\n",
      "def sum_even(numbers):\n",
      "   \"\"\"\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Initialize chat history in session state if it doesn't exist\n",
    "if 'chat_history' not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "st.title(\"AI-Powered Chatbot\")\n",
    "\n",
    "\n",
    "# Sidebar with instructions\n",
    "st.sidebar.title(\"Instructions\")\n",
    "st.sidebar.write(\"Ask any question, and the AI will try to provide informative and accurate answers.\")\n",
    "\n",
    "# User input field\n",
    "user_input = st.text_input(\"Type your question here:\", key=\"user_input\")\n",
    "\n",
    "# Submit button\n",
    "if st.button(\"Submit\"):\n",
    "    st.session_state.chat_history.append(\"You: \" + user_input)\n",
    "    with st.spinner('AI is thinking...'):\n",
    "        # Generate a response using the OpenAI API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[{\"role\": \"system\", \"content\": 'the AI will try to provide informative and accurate answers.'},\n",
    "                        {\"role\": \"user\", \"content\": user_input}\n",
    "              ],\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        ai_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        st.session_state.chat_history.append(\"AI: \" + ai_response)\n",
    "\n",
    "# Display the conversation history\n",
    "for message in st.session_state.chat_history:\n",
    "    st.text(message)\n",
    "\n",
    "# for git version control testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
